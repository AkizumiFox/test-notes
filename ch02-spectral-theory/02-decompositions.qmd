---
title: "2.2 Matrix Decompositions"
format:
  pdf:
    documentclass: article
    number-sections: true
    pdf-engine: lualatex
    geometry:
      - top=30mm
      - left=25mm
      - right=25mm
      - bottom=30mm
---

{{< include ../_common.qmd >}}

## Matrix Decompositions

### Introduction

Matrix decompositions factor a matrix into products of simpler matrices. These factorizations reveal structure and enable efficient computation.

### Spectral Decomposition

::: {#thm-spectral}
## Spectral Theorem
Let $\bA \in \R^{n \times n}$ be symmetric. Then $\bA$ can be decomposed as:
$$
\bA = \mathbf{Q} \mathbf{\Lambda} \mathbf{Q}^\top
$$
where $\mathbf{Q}$ is orthogonal (its columns are orthonormal eigenvectors of $\bA$) and $\mathbf{\Lambda} = \diag(\lambda_1, \ldots, \lambda_n)$ is diagonal (containing the eigenvalues).
:::

### Singular Value Decomposition

::: {#def-svd}
## Singular Value Decomposition (SVD)
Every matrix $\bA \in \R^{m \times n}$ can be factored as:
$$
\bA = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top
$$
where:

- $\mathbf{U} \in \R^{m \times m}$ is orthogonal (left singular vectors)
- $\mathbf{\Sigma} \in \R^{m \times n}$ is diagonal with non-negative entries (singular values)
- $\mathbf{V} \in \R^{n \times n}$ is orthogonal (right singular vectors)
:::

::: {#thm-svd-rank}
## SVD and Rank
The rank of $\bA$ equals the number of non-zero singular values.
:::

### Applications

::: {#exm-pca}
## Principal Component Analysis
PCA uses the SVD to find the directions of maximum variance in a dataset. If $\mathbf{X}$ is a centered data matrix, the principal components are the right singular vectors of $\mathbf{X}$.
:::

### Exercises

::: {#exr-pseudo-inverse}
## Moore-Penrose Pseudoinverse
Using the SVD $\bA = \mathbf{U} \mathbf{\Sigma} \mathbf{V}^\top$, show that the pseudoinverse is given by $\bA^+ = \mathbf{V} \mathbf{\Sigma}^+ \mathbf{U}^\top$.
:::
